
# **xAI – Deepfake Audio + X-ray XAI (Streamlit)**
---

> *HANOUZ Akram, JOUVIN Jules, JALAL Zakaria*<br>
> *DIA3*


> **DISCLAIMER**<br>
>
> **This README was crafted with AI assistance.**<br>
> **Some LLM tools was used as an assistant to write some of the functions throughout the project.**<br>
>
> The LLM tools used are: Raptor mini, GPT-5.1 Codex, GPT-5.2, GPT-5.2 Codex<br>
> Every text and code generated by those tools has been verified and reviewed by HANOUZ Akram, JOUVIN Jules and JALAL Zakaria


**Table of Contents**
<!-- TOC -->

- [xAI – Deepfake Audio + X-ray XAI Streamlit](#xai--deepfake-audio--x-ray-xai-streamlit)
    - [Datasets](#datasets)
    - [Installation et Lancement de l'Application](#installation-et-lancement-de-lapplication)
    - [Structure](#structure)
        - [Framework & modèles](#framework--mod%C3%A8les)
        - [Données supportées](#donn%C3%A9es-support%C3%A9es)
        - [Pré‑traitement important pour la “forme” des XAI](#pr%C3%A9%E2%80%91traitement-important-pour-la-forme-des-xai)
        - [XAI disponibles](#xai-disponibles)
        - [LLM explanation Ollama](#llm-explanation-ollama)
    - [Entraîner / générer les poids](#entra%C3%AEner--g%C3%A9n%C3%A9rer-les-poids)
    - [Notes pratiques](#notes-pratiques)

<!-- /TOC -->

---

## **Datasets**

Les **poids des différents modèles** utilisés peuvent être retrouvés sur le drive suivant:<br>
https://drive.google.com/drive/folders/1-gEu2oPO9F5EmUfNcMH-tjc8vzzKkHRx

Placez ce dossier `weights` dans la racine du projet.

Les datasets utilisés pour l'entrtraînement peuvent être retrouvé aux liens suivants (ils ne sont pas nécessaire pour lancer l'application):
- [FoR Dataset (https://www.kaggle.com)](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset?select=for-norm) (licence [LGPLv3](https://www.gnu.org/licenses/lgpl-3.0.html))
- [CheXpert-v1.0-small (https://www.kaggle.com)](https://www.kaggle.com/datasets/ashery/chexpert) (licence [CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0/))

Si vous souhaitez les spectogrames sous formes d'images, exécutez les 3 premières cellules du notebook `training_models.ipynb`

> Toute les informations sur comment le dataset a été formé sont décrites sur le lien.


## **Installation et Lancement de l'Application**


Créer un environement python virtuel (recommandé) et activez le:
```powershell
python -m venv .venv
. .\.venv\Scripts\activate    
```

Intaller `requirements.txt`.
```powershell
pip install -r requirements.txt
```

Installer torch. Choississez la version de CUDA compatible avec votre GPU. L'example ci-dessous utilise la 12.9.
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129
```

Puis depuis la racine du repo :
```powershell
py -m streamlit run app.py
```

Pré‑requis : Les poids doivent exister dans `./weights/` (les clés sont définies dans `project/app.py`).
Pré‑requis (LLM optionnel) : voir section **LLM explanation (Ollama)**.


## **Structure**

Ce repo contient un notebook d’entraînement (`project/training_models.ipynb`) et une app Streamlit `project/app.py`.

- `./weights/` : poids des modèles PyTorch exportés (`*.pt`).
- `./app.py` : UI Streamlit
- `./training_models.ipynb` : entraînement des modèles (PyTorch) et sauvegarde des poids.


### Framework & modèles

- Charge les modèles torchvision: 
  - VGG16/ResNet50/MobileNetV2 pour audio.
  - AlexNet/DenseNet121 pour X‑ray.
- Restaure les poids depuis `./weights/*.pt`.
- Prédiction binaire via **logit** + `sigmoid`.

### Données supportées

- Audio `.wav` → image (mel‑spectrogram) → modèle image.
- X‑ray (`png/jpg/jpeg`) → modèle image.

### Pré‑traitement (important pour la “forme” des XAI)

- Audio : génère le spectrogramme comme dans le repo de référence (**matplotlib → PNG → reload**) pour obtenir un rendu visuel plus proche.
- X‑ray : conversion en grayscale puis RGB + normalisation ImageNet (voir `./training_models.ipynb`).

### XAI disponibles

- **Grad‑CAM** (pytorch‑grad‑cam) : couche cible adaptée pour X‑ray (ReLU final) et mode debug optionnel.
- **LIME** (lime‑image) : segmentation superpixels + perturbations.
- **SHAP** (masker image + superpixels) : affichage rouge/vert (positif/négatif).
  - **Rouge** : régions qui poussent la prédiction vers la classe expliquée (contribution positive).
  - **Vert** : régions qui la tirent dans l’autre sens (contribution négative).


### LLM explanation (Ollama)

Une explication textuelle optionnelle peut être générée via **Ollama** (modèle local par défaut: `gpt-oss:20b`).

- **Aucune image/audio n'est envoyé au LLM** : l'explication est basée sur les probabilités prédites et des **résumés numériques légers** dérivés de Grad-CAM/LIME/SHAP.
- Aucun outil externe n'est appelé et aucune image/audio n'est transmise au modèle. Vous pouvez donc utiliser n'importe quel modèle local compatible avec Ollama (il n'est pas nécessaire de choisir un modèle « tool-enabled »).

Pré‑requis :
- Installer Ollama et démarrer le serveur local (par défaut `http://localhost:11434`).
  ```powershell
  ollama serve
  ```
- Télécharger le modèle que vous souhaitez utilisez. Exemple avec le modèle pas défaut :
  ```powershell
  ollama pull gpt-oss:20b
  ```

Utilisation dans l'app :
- Dans la sidebar : cocher **Enable Ollama explanation** et configurer `host` + `model`.
- Onglet **Result** → expander **LLM explanation (Ollama)** → bouton **Generate**.
- Des résumés textuelles des XAI sont optionnellement utilisés pour générer la réponse du LLM. Si vous les souhaitez, faites bien attention qu'ils ont fini d'être généré dans la section Compare.

Note technique : l'intégration Ollama est encapsulée dans `ollama_llm.py`.


## Entraîner / générer les poids


Si vous voulez rentrainer les modèles, ouvrez et exécutez `project/training_models.ipynb`.<br>
Les modèles sont sauvegardés dans `./weights/` et ensuite chargés par `./app.py`.


## Notes pratiques

- Les méthodes XAI peuvent donner des résultats différents (Grad‑CAM vs LIME vs SHAP) car elles n’expliquent pas exactement la même chose (gradients vs perturbations vs contributions moyennes).
- Pour Grad‑CAM X‑ray, une carte “vide” vient souvent d’une couche cible inadaptée (activations négatives + ReLU interne) : `project/app.py` cible donc une couche ReLU proche de la fin du backbone.
